{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to generate the ArchivalQA question-document pairings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#edit this path to point to the csv file containing questions and source document IDs \n",
    "archivalqa_path = '/data/archivalqa/ArchivalQA_val.csv'\n",
    "\n",
    "#edit this path to point to the directory containing the NYC corpus\n",
    "nyc_corpus_path = '/data/archivalqa/NYC_corpus/'\n",
    "save_path = '/data/archivalqa/ArchivalQA_paired/'\n",
    "\n",
    "archival_df = pd.read_csv(archivalqa_path)\n",
    "\n",
    "archival_df['doc_id'] = archival_df.apply(lambda x: x['para_id'].split('_')[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a xml file corresponding to an article in NYT corpus, return a dictionary with title, date, text, and doc_id\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    title = root.find('head/title')\n",
    "    if title is not None: title = title.text\n",
    "    # Get the publication date\n",
    "    year = root.find('head/meta[@name=\"publication_year\"]')\n",
    "    if year is not None:\n",
    "        year = year.attrib['content']\n",
    "    month = root.find('head/meta[@name=\"publication_month\"]')\n",
    "    if month is not None:\n",
    "        month = month.attrib['content']\n",
    "    day = root.find('head/meta[@name=\"publication_day_of_month\"]')\n",
    "    if day is not None:\n",
    "        day = day.attrib['content']\n",
    "\n",
    "    date = f\"{year}-{month}-{day}\"\n",
    "    full_text_block = root.find(\".//block[@class='full_text']\")\n",
    "    if full_text_block is None:\n",
    "        text_by_para = None\n",
    "    else:\n",
    "        text_by_para = [e.text for e in full_text_block.findall(\".//p\")]\n",
    "    doc_id = root.find(\".//doc-id\")\n",
    "    if doc_id is not None:\n",
    "        doc_id = doc_id.attrib['id-string']\n",
    "    return {'title': title, 'date': date, 'text': text_by_para, 'doc_id': doc_id}\n",
    "\n",
    "\n",
    "file_info = []\n",
    "for xml_file in tqdm(glob.glob(os.path.join(nyc_corpus_path, '**/*.xml'), recursive=True)):\n",
    "    file_info.append(parse_xml(xml_file))\n",
    "    \n",
    "corpus_df = pd.DataFrame(file_info)\n",
    "corpus_df.to_pickle(os.path.join(save_path, 'NYT_aggregated_info.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "archival_df = pd.merge(archival_df, corpus_df, on='doc_id', how='left')\n",
    "archival_df['para_num'] = archival_df.apply(lambda x: int(x['para_id'].split('_')[1]), axis=1)\n",
    "archival_df['ans_paragraph'] = archival_df.apply(lambda x: x['text'][x['para_num']], axis=1)\n",
    "archival_df['ans_text'] = archival_df.apply(lambda x: '\\n'.join(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 0s to date\n",
    "def pad_date(date):\n",
    "    date = date.split('-')\n",
    "    date = [d.zfill(2) for d in date]\n",
    "    return '-'.join(date)\n",
    "archival_df['date'] = archival_df.apply(lambda x: pad_date(x['date']), axis=1)\n",
    "archival_df['year'] =  archival_df.apply(lambda x: x['date'].split('-')[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_lt_df 12378\n",
      "qa_lt_val_df 5121\n",
      "train_df 21716\n",
      "val_df 5285\n",
      "test_df 8743\n"
     ]
    }
   ],
   "source": [
    "#qa_lt years 1987-1990 10k \n",
    "#qa_lt_val years 1991-1992 5k\n",
    "#meta_train years 1993-2001 20k\n",
    "#meta_val years 2002-2003 5k\n",
    "#test years 2004-2007 10k\n",
    "\n",
    "qa_lt_df = archival_df[archival_df['year'].isin(['1987', '1988', '1989', '1990'])]\n",
    "qa_lt_val_df = archival_df[archival_df['year'].isin(['1991', '1992'])]\n",
    "train_df = archival_df[archival_df['year'].isin(['1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001'])]\n",
    "val_df = archival_df[archival_df['year'].isin(['2002', '2003'])]\n",
    "test_df = archival_df[archival_df['year'].isin(['2004', '2005', '2006', '2007'])]\n",
    "\n",
    "print('qa_lt_df size:', len(qa_lt_df))\n",
    "print('qa_lt_val_df size:', len(qa_lt_val_df))\n",
    "print('train_df size:', len(train_df))\n",
    "print('val_df size:', len(val_df))\n",
    "print('test_df size:', len(test_df))\n",
    "\n",
    "qa_lt_df.to_csv(os.path.join(save_path + 'qa_lt.csv'), index=False)\n",
    "qa_lt_val_df.to_csv(os.path.join(save_path + 'qa_lt_val.csv'), index=False)\n",
    "train_df.to_csv(os.path.join(save_path + 'train.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(save_path + 'val.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(save_path + 'test.csv'), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lu456_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87393be873ca135c51aba14a87a04be6156862d5d568e542c4328788fd9e1558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
